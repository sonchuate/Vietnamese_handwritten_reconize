{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from common import imgShow, autoResize, orcPreprocess, encodeText, pad_listints\n",
    "from backbone.CNN import ConvBlock\n",
    "from transformer.RNN import RecBlock\n",
    "input_shape = [800, 100]\n",
    "from data.data_loader import data_loader\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import Model\n",
    "from common import totalCER, one_hot_decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load data\n",
    "images, labels, file_names = data_loader(prob = 10/103000)\n",
    "label_lengths = [len(i) for i in labels]\n",
    "labels = pad_listints([encodeText(label) for label in labels])\n",
    "\n",
    "\n",
    "val_images, val_labels, _ = data_loader(prob = 10/103000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "ss = [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]]\n",
    "ps = [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]]\n",
    "CHAR_LIST = sorted(\"-ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ\")\n",
    "vocab_len = len(CHAR_LIST)\n",
    "learning_rate=0.001\n",
    "learning_rate_decay = 0.00001\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "input_shape = (input_shape[1],input_shape[0])\n",
    "c = ConvBlock(input_shape=input_shape,hiden=128,ps=ps,ss=ss)\n",
    "r = RecBlock(vocab_len)\n",
    "input = Input(shape = input_shape)\n",
    "output = c.call(input)\n",
    "output = r.call(output)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lấy danh sách các biến cần cập nhật trong mô hình\n",
    "trainable_vars = model.trainable_variables\n",
    "\n",
    "# Sử dụng trình tối ưu hóa Adam\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Định nghĩa hàm mất mát và vòng lặp huấn luyện\n",
    "def ctc_loss(labels, label_length, predictions, num_frames = 188):\n",
    "    \n",
    "    \n",
    "    \"\"\"length is vocab length + 1\"\"\"\n",
    "    batch_size = labels.shape[0]\n",
    "    logit_length = [num_frames] * batch_size\n",
    "    predictions = tf.transpose(predictions, perm=[1, 0, 2])\n",
    "    return tf.nn.ctc_loss(\n",
    "      labels=labels,\n",
    "      logits=predictions,\n",
    "      label_length=label_length,\n",
    "      logit_length=logit_length,\n",
    "      blank_index=-1)\n",
    "\n",
    "def train_step(images, labels, label_lengths):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = ctc_loss(labels, label_lengths, predictions)\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tạo data set\n",
    "# Huấn luyện mô hình với trình tối ưu hóa Adam\n",
    "batch_size = 32\n",
    "# Tạo một dataset từ dữ liệu đào tạo và chia thành các mini-batch\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels, label_lengths))\n",
    "dataset = dataset.shuffle(buffer_size=100000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình với trình tối ưu hóa Adam\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "# Tạo một dataset từ dữ liệu đào tạo và chia thành các mini-batch\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels, label_lengths))\n",
    "dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels, label_lengths in dataset:\n",
    "        loss = train_step(images, labels, label_lengths)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')\n",
    "\n",
    "#train\n",
    "minCER = 1e15\n",
    "val_images = np.array(val_images)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels, label_lengths in dataset:\n",
    "        loss = train_step(images, labels, label_lengths)\n",
    "    print(f'Epoch {epoch + 1} , Loss: {loss.numpy()}')    \n",
    "    learning_rate*=(1 - learning_rate_decay)\n",
    "    optimizer.learning_rate.assign(learning_rate)\n",
    "    if(epoch % 2 == 0):\n",
    "        val_predictions = model.predict(val_images)\n",
    "        totalCER = totalCER(val_labels, one_hot_decoder(val_predictions))\n",
    "        if totalCER < minCER:\n",
    "            minCER = totalCER\n",
    "            model.save(\"myModel.h5\")\n",
    "        print(f'total CER = {totalCER}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
